"""
èµ›äº‹åˆ†ææ¨¡å— (Match Analysis Module)

ç”¨æˆ·åœºæ™¯ï¼š
- "åˆ†æä¸€ä¸‹æ›¼è”æœ€è¿‘çš„çŠ¶æ€"
- "æ›¼è”å’Œåˆ©ç‰©æµ¦è°æ›´å¼ºï¼Ÿ"
- "ä¸ºä»€ä¹ˆåˆ©ç‰©æµ¦æ’åç¬¬ä¸€ï¼Ÿ"
- "æ›¼è”å¯¹åˆ©ç‰©æµ¦è°ä¼šèµ¢ï¼Ÿ"

æ ¸å¿ƒåŠŸèƒ½ï¼š
- çƒé˜ŸçŠ¶æ€åˆ†æ
- ä¸¤é˜Ÿå¯¹æ¯”åˆ†æ
- æ’ååŸå› åˆ†æ
- æ¯”èµ›é¢„æµ‹åˆ†æ

ä½¿ç”¨å·²åˆ›å»ºçš„ï¼š
- DataAnalyzer: æ•°æ®æå–å’Œå¯¹æ¯”
- ReasoningEngine: æ·±åº¦æ¨ç†
"""
from __future__ import annotations

import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from src.agent.core.data_analyzer import DataAnalyzer
from src.agent.core.reasoning_engine import ReasoningEngine

# å¯¼å…¥ç°æœ‰å·¥å…·
from src.agent.tools.match_tool import match_tool
from src.agent.tools.standings_tool import standings_tool
from src.agent.tools.stats_tool import stats_tool
from src.agent.tools.prediction_tool import prediction_tool

# å¯¼å…¥LLMå®¢æˆ·ç«¯
from src.shared.llm_client import llm_client

logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    analysis_type: str  # "team_status" / "comparison" / "prediction" / "ranking_reason"
    analysis_text: str  # è‡ªç„¶è¯­è¨€åˆ†æ
    structured_data: Optional[Dict[str, Any]] = None  # ç»“æ„åŒ–æ•°æ®
    reasoning: Optional[Any] = None  # æ¨ç†ç»“æœ
    metadata: Dict[str, Any] = None  # å…ƒæ•°æ®


class MatchAnalysisModule:
    """
    èµ›äº‹åˆ†ææ¨¡å—

    ç‰¹ç‚¹ï¼šæ·±åº¦åˆ†æï¼Œä¸ä»…ç»™æ•°æ®ï¼Œè¿˜è¦è§£é‡Šä¸ºä»€ä¹ˆ
    """

    def __init__(self):
        self.data_analyzer = DataAnalyzer()
        self.reasoning_engine = ReasoningEngine()

    async def execute(self, query: str, entities: Dict[str, Any]) -> AnalysisResult:
        """
        æ‰§è¡Œåˆ†æ

        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            entities: æå–çš„å®ä½“

        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        logger.info(f"[MatchAnalysisModule] Processing query: {query}")
        logger.info(f"[MatchAnalysisModule] Entities: {entities}")

        # 1. è¯†åˆ«åˆ†æç±»å‹
        analysis_type = self._detect_analysis_type(query, entities)
        logger.info(f"[MatchAnalysisModule] Analysis type: {analysis_type}")

        # 2. æ”¶é›†æ•°æ®
        tool_results = await self._collect_data(entities, analysis_type)

        # 3. æ•°æ®åˆ†æï¼ˆç»“æ„åŒ–ï¼‰
        structured_data = self.data_analyzer.extract_structured_data(tool_results)

        # 4. æ·±åº¦æ¨ç†ï¼ˆå¦‚æœæ˜¯å¯¹æ¯”æˆ–é¢„æµ‹ï¼‰
        reasoning_result = None
        if analysis_type in ["comparison", "prediction"]:
            reasoning_result = await self._perform_reasoning(
                query,
                structured_data,
                analysis_type
            )

        # 5. ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€åˆ†æ
        analysis_text = await self._generate_analysis(
            query,
            structured_data,
            reasoning_result,
            analysis_type
        )

        return AnalysisResult(
            analysis_type=analysis_type,
            analysis_text=analysis_text,
            structured_data=structured_data,
            reasoning=reasoning_result,
            metadata={"query": query, "entities": entities}
        )

    def _detect_analysis_type(self, query: str, entities: Dict) -> str:
        """
        è¯†åˆ«åˆ†æç±»å‹

        ä¼˜å…ˆçº§ï¼š
        1. é¢„æµ‹ï¼ˆè°ä¼šèµ¢ï¼‰ > å¯¹æ¯” > çŠ¶æ€åˆ†æ > æ’ååŸå› 
        """
        query_lower = query.lower()

        # é¢„æµ‹åˆ†æ
        if any(k in query_lower for k in [
            "è°ä¼šèµ¢", "è°ä¼šè·èƒœ", "é¢„æµ‹", "ä¼šèµ¢å—", "èƒ½èµ¢å—",
            "æ¦‚ç‡", "èƒœç®—"
        ]):
            return "prediction"

        # å¯¹æ¯”åˆ†æï¼ˆä¸¤ä¸ªçƒé˜Ÿï¼‰
        if 'team_b' in entities and entities.get('team_b'):
            if any(k in query_lower for k in [
                "å¯¹æ¯”", "æ¯”è¾ƒ", "è°æ›´å¼º", "è°æ›´å¥½", "å·®è·",
                "vs", "pk", "å“ªä¸ª"
            ]):
                return "comparison"

        # æ’ååŸå› åˆ†æ
        if any(k in query_lower for k in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "æ€ä¹ˆå›äº‹"]):
            if any(k in query_lower for k in ["æ’å", "ç¬¬ä¸€", "è¿™ä¹ˆé«˜", "è¿™ä¹ˆä½"]):
                return "ranking_reason"

        # é»˜è®¤ï¼šçƒé˜ŸçŠ¶æ€åˆ†æ
        return "team_status"

    async def _collect_data(
        self,
        entities: Dict,
        analysis_type: str
    ) -> List[Dict[str, Any]]:
        """
        æ”¶é›†åˆ†ææ‰€éœ€çš„æ•°æ®

        Returns:
            å·¥å…·ç»“æœåˆ—è¡¨
        """
        tool_results = []

        try:
            if analysis_type == "team_status":
                # å•ä¸ªçƒé˜Ÿï¼šæˆ˜ç»© + æ’å
                team = entities.get('team') or entities.get('team_a')
                if team:
                    # æˆ˜ç»©
                    stats_result = await stats_tool.get_team_stats(team, last_n=10)
                    tool_results.append({
                        "tool_name": "StatsAnalysisTool",
                        "status": "success",
                        "source": "real",
                        "output": stats_result
                    })

                    # æ’å
                    standing_result = await standings_tool.get_team_standing(team)
                    tool_results.append({
                        "tool_name": "StandingsTool",
                        "status": "success",
                        "source": "real",
                        "output": standing_result
                    })

            elif analysis_type in ["comparison", "prediction"]:
                # ä¸¤ä¸ªçƒé˜Ÿï¼šå„è‡ªçš„æˆ˜ç»© + æ’å
                team_a = entities.get('team_a') or entities.get('team')
                team_b = entities.get('team_b')

                if team_a:
                    stats_a = await stats_tool.get_team_stats(team_a, last_n=10)
                    standing_a = await standings_tool.get_team_standing(team_a)

                    tool_results.append({
                        "tool_name": "StatsAnalysisTool",
                        "status": "success",
                        "source": "real",
                        "output": stats_a
                    })
                    tool_results.append({
                        "tool_name": "StandingsTool",
                        "status": "success",
                        "source": "real",
                        "output": standing_a
                    })

                if team_b:
                    stats_b = await stats_tool.get_team_stats(team_b, last_n=10)
                    standing_b = await standings_tool.get_team_standing(team_b)

                    tool_results.append({
                        "tool_name": "StatsAnalysisTool",
                        "status": "success",
                        "source": "real",
                        "output": stats_b
                    })
                    tool_results.append({
                        "tool_name": "StandingsTool",
                        "status": "success",
                        "source": "real",
                        "output": standing_b
                    })

                # å¦‚æœæ˜¯é¢„æµ‹ï¼Œè°ƒç”¨é¢„æµ‹å·¥å…·
                if analysis_type == "prediction" and team_a and team_b:
                    try:
                        pred_result = await prediction_tool.predict_match(
                            home_team_name=team_a,
                            away_team_name=team_b
                        )
                        tool_results.append({
                            "tool_name": "PredictionTool",
                            "status": "success",
                            "source": "real",
                            "output": pred_result
                        })
                    except Exception as e:
                        logger.warning(f"Prediction tool failed: {e}")

            elif analysis_type == "ranking_reason":
                # æ’ååŸå› ï¼šæˆ˜ç»© + æ’å + è”èµ›æ•´ä½“æƒ…å†µ
                team = entities.get('team') or entities.get('team_a')
                if team:
                    stats_result = await stats_tool.get_team_stats(team, last_n=10)
                    standing_result = await standings_tool.get_team_standing(team)

                    tool_results.append({
                        "tool_name": "StatsAnalysisTool",
                        "status": "success",
                        "source": "real",
                        "output": stats_result
                    })
                    tool_results.append({
                        "tool_name": "StandingsTool",
                        "status": "success",
                        "source": "real",
                        "output": standing_result
                    })

        except Exception as e:
            logger.error(f"[MatchAnalysisModule] Data collection failed: {e}", exc_info=True)

        return tool_results

    async def _perform_reasoning(
        self,
        query: str,
        structured_data: Dict,
        analysis_type: str
    ) -> Optional[Any]:
        """
        æ‰§è¡Œæ·±åº¦æ¨ç†ï¼ˆä½¿ç”¨ReasoningEngineï¼‰

        åªåœ¨å¯¹æ¯”å’Œé¢„æµ‹åœºæ™¯ä¸‹ä½¿ç”¨
        """
        if len(structured_data) < 2:
            logger.warning("[MatchAnalysisModule] Not enough data for reasoning")
            return None

        try:
            teams = list(structured_data.keys())
            team_a_data = structured_data[teams[0]]
            team_b_data = structured_data[teams[1]]

            # å¤šç»´åº¦å¯¹æ¯”
            comparisons = self.data_analyzer.multi_dimensional_comparison(
                team_a_data,
                team_b_data
            )

            # å‡†å¤‡æ¨ç†æ•°æ®
            reasoning_data = self.data_analyzer.prepare_for_reasoning(
                structured_data,
                comparisons
            )

            # è°ƒç”¨æ¨ç†å¼•æ“
            reasoning_result = await self.reasoning_engine.analyze_match_prediction(
                query=query,
                structured_data=reasoning_data,
                comparisons=comparisons
            )

            return reasoning_result

        except Exception as e:
            logger.error(f"[MatchAnalysisModule] Reasoning failed: {e}", exc_info=True)
            return None

    async def _generate_analysis(
        self,
        query: str,
        structured_data: Dict,
        reasoning: Optional[Any],
        analysis_type: str
    ) -> str:
        """
        ç”¨LLMç”Ÿæˆæ·±åº¦åˆ†ææ–‡æœ¬
        """
        system_prompt = self._get_system_prompt(analysis_type)
        user_prompt = self._build_user_prompt(
            query,
            structured_data,
            reasoning,
            analysis_type
        )

        try:
            analysis = await llm_client.generate(system_prompt, user_prompt)
            return analysis

        except Exception as e:
            logger.error(f"[MatchAnalysisModule] LLM generation failed: {e}")
            # é™çº§ï¼šè¿”å›ç®€å•çš„æ•°æ®æ‘˜è¦
            return self._fallback_analysis(structured_data, reasoning)

    def _get_system_prompt(self, analysis_type: str) -> str:
        """æ ¹æ®åˆ†æç±»å‹è·å–system prompt"""

        base_prompt = """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è¶³çƒæ•°æ®åˆ†æå¸ˆã€‚

åˆ†æè¦æ±‚ï¼š
1. **å¤šç»´åº¦**ï¼šä»æ’åã€çŠ¶æ€ã€è¿›æ”»ã€é˜²å®ˆç­‰å¤šä¸ªè§’åº¦åˆ†æ
2. **æœ‰é€»è¾‘**ï¼šæ•°æ® â†’ åˆ†æ â†’ ç»“è®ºï¼Œæ¸…æ™°çš„æ¨ç†é“¾
3. **é‡åŒ–è¡¨è¾¾**ï¼šç”¨å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”ï¼Œä¸è¦æ¨¡ç³Šè¡¨è¾¾
4. **å…³é”®æ´å¯Ÿ**ï¼šæ‰¾å‡ºæœ€æ ¸å¿ƒçš„1-2ä¸ªé—®é¢˜æˆ–ä¼˜åŠ¿

è¾“å‡ºæ ¼å¼ï¼š
- ä½¿ç”¨emojiå¢å¼ºå¯è¯»æ€§
- åˆ†å±‚å±•ç¤ºï¼ˆæ ¸å¿ƒç»“è®º â†’ æ•°æ®æ”¯æ’‘ â†’ è¯¦ç»†åˆ†æï¼‰
- ç®€æ´æœ‰åŠ›ï¼Œé¿å…åºŸè¯
"""

        type_specific = {
            "team_status": """
ç‰¹å®šä»»åŠ¡ï¼šåˆ†æçƒé˜Ÿå½“å‰çŠ¶æ€

è¾“å‡ºç»“æ„ï¼š
1. æ ¸å¿ƒç»“è®ºï¼ˆä¸€å¥è¯æ¦‚æ‹¬çŠ¶æ€ï¼‰
2. æ•°æ®æ”¯æ’‘ï¼ˆ3-5ä¸ªå…³é”®æ•°æ®ç‚¹ï¼‰
3. æ ¸å¿ƒé—®é¢˜æˆ–ä¼˜åŠ¿ï¼ˆ1-2ä¸ªï¼‰
4. ç®€è¦å»ºè®®ï¼ˆå¯é€‰ï¼‰
""",
            "comparison": """
ç‰¹å®šä»»åŠ¡ï¼šå¯¹æ¯”ä¸¤æ”¯çƒé˜Ÿ

è¾“å‡ºç»“æ„ï¼š
1. æ ¸å¿ƒç»“è®ºï¼ˆè°æ›´å¼ºï¼Œä¼˜åŠ¿å¤šå¤§ï¼‰
2. å¤šç»´åº¦å¯¹æ¯”ï¼ˆæ’åã€çŠ¶æ€ã€è¿›æ”»ã€é˜²å®ˆï¼‰
3. å„è‡ªä¼˜åŠ£åŠ¿
4. ç»¼åˆè¯„ä»·
""",
            "prediction": """
ç‰¹å®šä»»åŠ¡ï¼šé¢„æµ‹æ¯”èµ›ç»“æœ

è¾“å‡ºç»“æ„ï¼š
1. æ ¸å¿ƒé¢„æµ‹ï¼ˆè°ä¼šèµ¢ï¼Œæ¦‚ç‡å¤šå°‘ï¼‰
2. æ•°æ®æ”¯æ’‘ï¼ˆä¸ºä»€ä¹ˆè¿™ä¹ˆé¢„æµ‹ï¼‰
3. é£é™©å› ç´ ï¼ˆå¯èƒ½æ”¹å˜ç»“æœçš„å› ç´ ï¼‰
4. å»ºè®®ï¼ˆå¯é€‰ï¼‰
""",
            "ranking_reason": """
ç‰¹å®šä»»åŠ¡ï¼šè§£é‡Šæ’ååŸå› 

è¾“å‡ºç»“æ„ï¼š
1. æ ¸å¿ƒåŸå› ï¼ˆæœ€ä¸»è¦çš„1-2ä¸ªå› ç´ ï¼‰
2. æ•°æ®è¯æ®ï¼ˆç”¨æ•°æ®è¯´æ˜ï¼‰
3. è¶‹åŠ¿é¢„æµ‹ï¼ˆæ’åä¼šä¸Šå‡è¿˜æ˜¯ä¸‹é™ï¼‰
"""
        }

        return base_prompt + type_specific.get(analysis_type, "")

    def _build_user_prompt(
        self,
        query: str,
        structured_data: Dict,
        reasoning: Optional[Any],
        analysis_type: str
    ) -> str:
        """æ„å»ºuser prompt"""

        prompt = f"ç”¨æˆ·é—®ï¼š{query}\n\n"

        # æ·»åŠ ç»“æ„åŒ–æ•°æ®
        prompt += "å¯ç”¨æ•°æ®ï¼š\n"
        for team_name, team_data in structured_data.items():
            prompt += f"\nã€{team_name}ã€‘\n"

            rank_text = str(team_data.rank) if team_data.rank else 'æœªçŸ¥'
            prompt += f"- æ’åï¼š{rank_text}\n"

            points_text = str(team_data.points) if team_data.points else 'æœªçŸ¥'
            prompt += f"- ç§¯åˆ†ï¼š{points_text}\n"

            win_rate_text = f'{team_data.win_rate:.1%}' if team_data.win_rate is not None else 'æœªçŸ¥'
            prompt += f"- èƒœç‡ï¼š{win_rate_text}\n"

            recent_win_rate_text = f'{team_data.recent_win_rate:.1%}' if team_data.recent_win_rate is not None else 'æœªçŸ¥'
            prompt += f"- è¿‘æœŸèƒœç‡ï¼š{recent_win_rate_text}\n"

        # æ·»åŠ æ¨ç†ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if reasoning:
            prompt += f"\næ¨ç†åˆ†æï¼š\n"
            prompt += f"- ç»“è®ºï¼š{reasoning.conclusion}\n"
            prompt += f"- ç½®ä¿¡åº¦ï¼š{reasoning.overall_confidence:.1%}\n"
            prompt += f"- å› æœé“¾ï¼š\n"
            for chain in reasoning.causal_chain[:3]:  # åªå–å‰3ä¸ª
                prompt += f"  * {chain}\n"

        prompt += "\nè¯·è¿›è¡Œæ·±åº¦åˆ†æã€‚"

        return prompt

    def _fallback_analysis(
        self,
        structured_data: Dict,
        reasoning: Optional[Any]
    ) -> str:
        """é™çº§åˆ†æï¼ˆLLMä¸å¯ç”¨æ—¶ï¼‰"""
        output = "ğŸ“Š æ•°æ®åˆ†æ\n\n"

        for team_name, team_data in structured_data.items():
            output += f"ã€{team_name}ã€‘\n"
            output += f"- æ’åï¼šç¬¬{team_data.rank}ä½\n" if team_data.rank else ""
            output += f"- ç§¯åˆ†ï¼š{team_data.points}åˆ†\n" if team_data.points else ""
            output += f"- èƒœç‡ï¼š{team_data.win_rate:.1%}\n" if team_data.win_rate else ""
            output += "\n"

        if reasoning:
            output += f"\nğŸ’¡ æ¨ç†ç»“è®º\n"
            output += f"{reasoning.conclusion}\n"
            output += f"ç½®ä¿¡åº¦ï¼š{reasoning.overall_confidence:.1%}\n"

        return output
