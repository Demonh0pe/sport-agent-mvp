"""
æµ‹è¯•LLMå®¢æˆ·ç«¯çš„æ¨¡å‹åˆ‡æ¢èƒ½åŠ›

æ¼”ç¤ºï¼š
1. è‡ªåŠ¨é€‰æ‹©åç«¯
2. æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹
3. ä¸´æ—¶ä½¿ç”¨ä¸åŒæ¨¡å‹
4. æŸ¥çœ‹å¯ç”¨æ¨¡å‹
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.shared.llm_client_v2 import UnifiedLLMClient, LLMBackend


async def test_auto_backend():
    """æµ‹è¯•1: è‡ªåŠ¨é€‰æ‹©åç«¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•1: è‡ªåŠ¨é€‰æ‹©åç«¯")
    print("=" * 60)

    client = UnifiedLLMClient()
    print(f"âœ… è‡ªåŠ¨é€‰æ‹©çš„åç«¯: {client.backend.value}")
    print(f"âœ… é»˜è®¤æ¨¡å‹: {client.default_model}")

    # æµ‹è¯•ç”Ÿæˆ
    response = await client.generate(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªè¶³çƒåˆ†æåŠ©æ‰‹ã€‚",
        user_prompt="ç®€å•ä»‹ç»ä¸€ä¸‹è‹±è¶…è”èµ›ã€‚ï¼ˆæ§åˆ¶åœ¨50å­—å†…ï¼‰"
    )

    print(f"\nğŸ“ ç”Ÿæˆç»“æœ:\n{response}")


async def test_switch_models():
    """æµ‹è¯•2: åˆ‡æ¢ä¸åŒæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: åˆ‡æ¢ä¸åŒæ¨¡å‹")
    print("=" * 60)

    # å¦‚æœOllamaå¯ç”¨ï¼Œæµ‹è¯•ä¸åŒå¤§å°çš„æ¨¡å‹
    try:
        import ollama

        # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        models_info = ollama.list()
        available_models = [m['name'] for m in models_info.get('models', [])]

        print(f"ğŸ“‹ æœ¬åœ°å¯ç”¨æ¨¡å‹: {available_models}")

        # æµ‹è¯•ä¸åŒæ¨¡å‹
        client = UnifiedLLMClient(backend="ollama")

        for model_name in available_models[:2]:  # åªæµ‹è¯•å‰2ä¸ª
            print(f"\nğŸ”„ åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")

            response = await client.generate(
                system_prompt="ä½ æ˜¯è¶³çƒä¸“å®¶ã€‚",
                user_prompt="ç”¨ä¸€å¥è¯è¯„ä»·æ›¼è”ã€‚",
                model=model_name  # ä¸´æ—¶åˆ‡æ¢
            )

            print(f"   å›ç­”: {response}")

    except ImportError:
        print("âš ï¸  Ollamaæœªå®‰è£…ï¼Œè·³è¿‡æœ¬åœ°æ¨¡å‹æµ‹è¯•")


async def test_backend_switching():
    """æµ‹è¯•3: åç«¯åˆ‡æ¢"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: åç«¯åˆ‡æ¢")
    print("=" * 60)

    client = UnifiedLLMClient()

    # æ–¹å¼1: åˆå§‹åŒ–æ—¶æŒ‡å®š
    print("\næ–¹å¼1: åˆå§‹åŒ–æ—¶æŒ‡å®šåç«¯")
    client_ollama = UnifiedLLMClient(backend="ollama", model="qwen2.5:7b")
    print(f"   âœ… åˆ›å»ºäº†Ollamaå®¢æˆ·ç«¯: {client_ollama.default_model}")

    # æ–¹å¼2: åŠ¨æ€åˆ‡æ¢
    print("\næ–¹å¼2: åŠ¨æ€åˆ‡æ¢åç«¯")
    client.switch_backend("ollama", model="qwen2.5:7b")
    print(f"   âœ… åˆ‡æ¢åˆ°: {client.backend.value} - {client.default_model}")


async def test_fallback():
    """æµ‹è¯•4: è‡ªåŠ¨é™çº§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: è‡ªåŠ¨é™çº§æœºåˆ¶")
    print("=" * 60)

    # æµ‹è¯•Ollamaä¸å¯ç”¨æ—¶çš„é™çº§
    client = UnifiedLLMClient(backend="ollama")

    print("ğŸ“ æµ‹è¯•åœºæ™¯: Ollamaè°ƒç”¨å¤±è´¥æ—¶ä¼šè‡ªåŠ¨é™çº§åˆ°API")
    print("   ï¼ˆè¿™éœ€è¦åœ¨å®é™…è¿è¡Œä¸­è§¦å‘ï¼Œæ­¤å¤„ä»…è¯´æ˜æœºåˆ¶ï¼‰")
    print("\n   é™çº§ç­–ç•¥:")
    print("   1. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollamaï¼ˆå¿«é€Ÿã€å…è´¹ï¼‰")
    print("   2. Ollamaå¤±è´¥ â†’ è‡ªåŠ¨é™çº§åˆ°DeepSeek API")
    print("   3. APIä¹Ÿå¤±è´¥ â†’ è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯")


async def test_model_listing():
    """æµ‹è¯•5: åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5: åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    print("=" * 60)

    # Ollamaæœ¬åœ°æ¨¡å‹
    try:
        client_ollama = UnifiedLLMClient(backend="ollama")
        ollama_models = client_ollama.get_available_models()
        print(f"\nğŸ“‹ Ollamaæœ¬åœ°æ¨¡å‹:")
        for model in ollama_models:
            print(f"   - {model}")
    except Exception as e:
        print(f"âš ï¸  Ollamaä¸å¯ç”¨: {e}")

    # DeepSeek APIæ¨¡å‹
    print(f"\nğŸ“‹ DeepSeek APIæ¨¡å‹:")
    print(f"   - deepseek-chat")
    print(f"   - deepseek-coder")

    # OpenAI APIæ¨¡å‹
    print(f"\nğŸ“‹ OpenAI APIæ¨¡å‹:")
    print(f"   - gpt-4o")
    print(f"   - gpt-4o-mini")
    print(f"   - gpt-3.5-turbo")


async def test_configurations():
    """æµ‹è¯•6: ä¸åŒé…ç½®æ–¹å¼"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•6: å¤šç§é…ç½®æ–¹å¼")
    print("=" * 60)

    # æ–¹å¼1: ç¯å¢ƒå˜é‡
    print("\næ–¹å¼1: ç¯å¢ƒå˜é‡ï¼ˆæœ€çµæ´»ï¼‰")
    print("""
    export LLM_BACKEND=ollama
    export LLM_MODEL=qwen2.5:7b
    """)

    # æ–¹å¼2: ä»£ç æŒ‡å®š
    print("\næ–¹å¼2: ä»£ç æŒ‡å®š")
    print("""
    client = UnifiedLLMClient(backend="ollama", model="qwen2.5:14b")
    """)

    # æ–¹å¼3: é…ç½®æ–‡ä»¶ï¼ˆæœªæ¥å®ç°ï¼‰
    print("\næ–¹å¼3: é…ç½®æ–‡ä»¶ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰")
    print("""
    # config/llm.yaml
    backend: ollama
    model: qwen2.5:7b
    fallback_backend: deepseek
    """)


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸš€ LLMå®¢æˆ·ç«¯æ¨¡å‹åˆ‡æ¢èƒ½åŠ›æµ‹è¯•")
    print("=" * 60)

    try:
        await test_auto_backend()
    except Exception as e:
        print(f"âŒ æµ‹è¯•1å¤±è´¥: {e}")

    try:
        await test_switch_models()
    except Exception as e:
        print(f"âš ï¸  æµ‹è¯•2è·³è¿‡: {e}")

    try:
        await test_backend_switching()
    except Exception as e:
        print(f"âŒ æµ‹è¯•3å¤±è´¥: {e}")

    try:
        await test_fallback()
    except Exception as e:
        print(f"âŒ æµ‹è¯•4å¤±è´¥: {e}")

    try:
        await test_model_listing()
    except Exception as e:
        print(f"âŒ æµ‹è¯•5å¤±è´¥: {e}")

    try:
        await test_configurations()
    except Exception as e:
        print(f"âŒ æµ‹è¯•6å¤±è´¥: {e}")

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
